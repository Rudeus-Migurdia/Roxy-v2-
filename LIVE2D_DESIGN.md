# nakari Live2D è¯¦ç»†è®¾è®¡æ–‡æ¡£

## ä¸€ã€é€šä¿¡åè®®è®¾è®¡

### 1.1 WebSocket æ¶ˆæ¯æ ¼å¼

æ‰€æœ‰ WebSocket æ¶ˆæ¯ä½¿ç”¨ JSON æ ¼å¼ï¼š

```typescript
interface NakariMessage {
  version: "1.0";
  type: MessageType;
  timestamp: number;
  payload: unknown;
}

type MessageType =
  | "state"          // çŠ¶æ€å˜åŒ–
  | "reply"          // æ–‡æœ¬å›å¤
  | "audio_chunk"    // éŸ³é¢‘å—
  | "emotion"        // è¡¨æƒ…å˜åŒ–
  | "motion"         // åŠ¨ä½œè§¦å‘
  | "user_text"      // ç”¨æˆ·è¾“å…¥
  | "error"          // é”™è¯¯ä¿¡æ¯
  | "ping"           // å¿ƒè·³
  | "pong";          // å¿ƒè·³å“åº”
```

### 1.2 æ¶ˆæ¯ç±»å‹è¯¦è§£

#### State æ¶ˆæ¯ (nakari â†’ frontend)

```typescript
interface StateMessage {
  version: "1.0";
  type: "state";
  timestamp: number;
  payload: {
    state: "thinking" | "speaking" | "idle" | "processing";
    event_id?: string;
  };
}

// ç¤ºä¾‹
{
  "version": "1.0",
  "type": "state",
  "timestamp": 1708435200000,
  "payload": {
    "state": "thinking",
    "event_id": "abc123def456"
  }
}
```

#### Reply æ¶ˆæ¯ (nakari â†’ frontend)

```typescript
interface ReplyMessage {
  version: "1.0";
  type: "reply";
  timestamp: number;
  payload: {
    content: string;
    event_id?: string;
    metadata?: {
      emotion?: string;
      tool_calls?: ToolCall[];
    };
  };
}

// ç¤ºä¾‹
{
  "version": "1.0",
  "type": "reply",
  "timestamp": 1708435200000,
  "payload": {
    "content": "ä½ å¥½ï¼æˆ‘æ˜¯ nakariã€‚",
    "event_id": "abc123def456",
    "metadata": {
      "emotion": "happy"
    }
  }
}
```

#### Audio Chunk æ¶ˆæ¯ (nakari â†’ frontend)

```typescript
interface AudioChunkMessage {
  version: "1.0";
  type: "audio_chunk";
  timestamp: number;
  payload: {
    chunk_id: string;       // éŸ³é¢‘å— ID
    sequence: number;       // åºåˆ—å·
    data: string;           // base64 ç¼–ç çš„éŸ³é¢‘æ•°æ®
    format: "mp3" | "wav";
    sample_rate: number;
    channels: number;
    is_final: boolean;      // æ˜¯å¦ä¸ºæœ€åä¸€å—
  };
}
```

#### User Text æ¶ˆæ¯ (frontend â†’ nakari)

```typescript
interface UserTextMessage {
  version: "1.0";
  type: "user_text";
  timestamp: number;
  payload: {
    content: string;
    metadata?: {
      source?: "web" | "mobile";
    };
  };
}
```

#### Emotion æ¶ˆæ¯

```typescript
interface EmotionMessage {
  version: "1.0";
  type: "emotion";
  timestamp: number;
  payload: {
    emotion: "neutral" | "happy" | "sad" | "angry" | "surprised";
    intensity: number;       // 0.0 - 1.0
    duration?: number;       // æŒç»­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
  };
}
```

#### Motion æ¶ˆæ¯

```typescript
interface MotionMessage {
  version: "1.0";
  type: "motion";
  timestamp: number;
  payload: {
    motion: string;
    group?: string;
    loop?: boolean;
  };
}
```

### 1.3 å¿ƒè·³æœºåˆ¶

```typescript
// å®¢æˆ·ç«¯æ¯ 30 ç§’å‘é€ ping
interface PingMessage {
  version: "1.0";
  type: "ping";
  timestamp: number;
  payload: null;
}

// æœåŠ¡å™¨å“åº” pong
interface PongMessage {
  version: "1.0";
  type: "pong";
  timestamp: number;
  payload: {
    server_time: number;
  };
}
```

## äºŒã€API æ¥å£è®¾è®¡

### 2.1 HTTP API

```typescript
// GET /api/health - å¥åº·æ£€æŸ¥
interface HealthResponse {
  status: "ok" | "error";
  version: string;
  uptime: number;
  websocket_url: string;
}

// GET /api/config - è·å–é…ç½®
interface ConfigResponse {
  live2d: {
    model_name: string;
    available_models: string[];
    lip_sync_enabled: boolean;
    auto_idle: boolean;
  };
  audio: {
    sample_rate: number;
    chunk_size: number;
    format: string;
  };
}

// POST /api/tts/test - TTS æµ‹è¯•
interface TTS TestRequest {
  text: string;
  speak: boolean;
}

interface TTS TestResponse {
  success: boolean;
  audio_url?: string;
}

// GET /api/models - è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
interface ModelsResponse {
  models: Array<{
    name: string;
    display_name: string;
    thumbnail?: string;
  }>;
}

// GET /api/emotions - è·å–å¯ç”¨è¡¨æƒ…åˆ—è¡¨
interface EmotionsResponse {
  emotions: Array<{
    id: string;
    name: string;
    description: string;
  }>;
}
```

### 2.2 WebSocket API

```typescript
// è¿æ¥ URL
ws://localhost:8000/api/ws

// è¿æ¥æŸ¥è¯¢å‚æ•°
?client_id=xxx&model=haru&lip_sync=true
```

## ä¸‰ã€æ•°æ®ç»“æ„è®¾è®¡

### 3.1 å‰ç«¯çŠ¶æ€ç®¡ç†

```typescript
interface NakariState {
  // è¿æ¥çŠ¶æ€
  connection: {
    status: "connecting" | "connected" | "disconnected" | "error";
    ws: WebSocket | null;
    lastPing: number;
  };

  // Live2D çŠ¶æ€
  live2d: {
    model: string;
    isLoaded: boolean;
    currentMotion: string | null;
    currentEmotion: string | null;
    parameters: Record<string, number>;
  };

  // nakari çŠ¶æ€
  nakari: {
    state: "idle" | "thinking" | "speaking" | "processing";
    currentEvent: string | null;
  };

  // å¯¹è¯çŠ¶æ€
  conversation: {
    messages: Array<{
      id: string;
      role: "user" | "assistant";
      content: string;
      timestamp: number;
      emotion?: string;
    }>;
  };

  // éŸ³é¢‘çŠ¶æ€
  audio: {
    isPlaying: boolean;
    volume: number;
    currentChunk: string | null;
  };
}
```

### 3.2 Live2D æ¨¡å‹å…ƒæ•°æ®

```typescript
interface Live2DModelMetadata {
  name: string;
  display_name: string;
  version: string;

  // æ¨¡å‹æ–‡ä»¶
  model_file: string;
  texture_dir: string;

  // å‚æ•°å®šä¹‰
  parameters: Array<{
    name: string;
    id: string;
    min: number;
    max: number;
    default: number;
  }>;

  // åŠ¨ä½œç»„
  motions: {
    [group: string]: Array<{
      file: string;
      name: string;
      sound?: string;
    }>;
  };

  // è¡¨æƒ…é¢„è®¾
  expressions?: Array<{
    name: string;
    parameters: Record<string, number>;
  }>;
}
```

## å››ã€æ ¸å¿ƒç±»è®¾è®¡

### 4.1 WebSocketManager (åç«¯)

```python
from __future__ import annotations
import asyncio
import json
from typing import Dict, Set
from fastapi import WebSocket
import structlog

class WebSocketManager:
    """WebSocket è¿æ¥ç®¡ç†å™¨"""

    def __init__(self) -> None:
        self._connections: Dict[str, WebSocket] = {}
        self._subscribers: Dict[str, Set[str]] = {}  # topic -> client_ids
        self._log = structlog.get_logger("websocket")

    async def connect(self, client_id: str, ws: WebSocket) -> None:
        """æ¥å—æ–°è¿æ¥"""
        await ws.accept()
        self._connections[client_id] = ws
        self._log.info("client_connected", client_id=client_id)

    def disconnect(self, client_id: str) -> None:
        """æ–­å¼€è¿æ¥"""
        if client_id in self._connections:
            del self._connections[client_id]
            # æ¸…ç†è®¢é˜…
            for topic, subscribers in self._subscribers.items():
                subscribers.discard(client_id)
            self._log.info("client_disconnected", client_id=client_id)

    async def send(self, client_id: str, message: dict) -> bool:
        """å‘é€æ¶ˆæ¯ç»™æŒ‡å®šå®¢æˆ·ç«¯"""
        ws = self._connections.get(client_id)
        if ws is None:
            return False

        try:
            await ws.send_json(message)
            return True
        except Exception as e:
            self._log.warning("send_failed", client_id=client_id, error=str(e))
            self.disconnect(client_id)
            return False

    async def broadcast(self, message: dict, topic: str | None = None) -> None:
        """å¹¿æ’­æ¶ˆæ¯"""
        if topic:
            clients = self._subscribers.get(topic, set())
        else:
            clients = set(self._connections.keys())

        for client_id in clients:
            await self.send(client_id, message)

    def subscribe(self, client_id: str, topic: str) -> None:
        """è®¢é˜…ä¸»é¢˜"""
        if topic not in self._subscribers:
            self._subscribers[topic] = set()
        self._subscribers[topic].add(client_id)

    def unsubscribe(self, client_id: str, topic: str) -> None:
        """å–æ¶ˆè®¢é˜…"""
        if topic in self._subscribers:
            self._subscribers[topic].discard(client_id)

    @property
    def connection_count(self) -> int:
        return len(self._connections)
```

### 4.2 AudioBroadcaster (åç«¯)

```python
from __future__ import annotations
import asyncio
import base64
from collections import deque
from typing import TYPE_CHECKING
import structlog

if TYPE_CHECKING:
    from .websocket import WebSocketManager

class AudioBroadcaster:
    """éŸ³é¢‘æµå¹¿æ’­å™¨"""

    def __init__(self, ws_manager: WebSocketManager, chunk_size: int = 4096):
        self._ws_manager = ws_manager
        self._chunk_size = chunk_size
        self._current_sequence = 0
        self._log = structlog.get_logger("audio_broadcaster")

    async def broadcast(self, audio_data: bytes, format: str = "mp3") -> None:
        """å¹¿æ’­éŸ³é¢‘å—"""
        # åˆ†å—
        chunks = [audio_data[i:i+self._chunk_size]
                  for i in range(0, len(audio_data), self._chunk_size)]

        for i, chunk in enumerate(chunks):
            message = {
                "version": "1.0",
                "type": "audio_chunk",
                "timestamp": asyncio.get_event_loop().time(),
                "payload": {
                    "chunk_id": f"{self._current_sequence}_{i}",
                    "sequence": self._current_sequence,
                    "data": base64.b64encode(chunk).decode(),
                    "format": format,
                    "is_final": (i == len(chunks) - 1),
                }
            }
            await self._ws_manager.broadcast(message)
            # å°å»¶è¿Ÿé¿å…é˜»å¡
            await asyncio.sleep(0.001)

        self._current_sequence += 1
```

### 4.3 Live2DRenderer (å‰ç«¯)

```typescript
import * as PIXI from 'pixi.js';
import { LIVE2DCubismSDK } from './live2d.min';

export class Live2DRenderer {
  private app: PIXI.Application;
  private model: LAppModel | null = null;
  private canvas: HTMLCanvasElement;
  private motionManager: MotionManager;
  private paramController: ParamController;

  constructor(canvas: HTMLCanvasElement) {
    this.canvas = canvas;
    this.motionManager = new MotionManager();
    this.paramController = new ParamController();

    // åˆå§‹åŒ– PixiJS
    this.app = new PIXI.Application({
      view: canvas,
      backgroundColor: 0x000000,
      resolution: window.devicePixelRatio || 1,
      autoDensity: true,
    });

    // çª—å£å¤§å°å˜åŒ–
    window.addEventListener('resize', () => this.onResize());
  }

  async loadModel(modelPath: string): Promise<void> {
    try {
      // åŠ è½½æ¨¡å‹
      const model = await LAppModel.load(modelPath);
      this.model = model;

      // è®¾ç½®å‚æ•°æ§åˆ¶å™¨
      this.paramController.setModel(model);

      // æ·»åŠ åˆ°èˆå°
      this.app.stage.addChild(model);

      // å¯åŠ¨æ¸²æŸ“å¾ªç¯
      this.startRenderLoop();

    } catch (error) {
      console.error('Failed to load Live2D model:', error);
      throw error;
    }
  }

  private startRenderLoop(): void {
    const ticker = () => {
      if (this.model) {
        this.model.update();
        this.motionManager.update(this.model);
      }
      requestAnimationFrame(ticker);
    };
    ticker();
  }

  setParam(paramName: string, value: number): void {
    this.paramController?.set(paramName, value);
  }

  playMotion(group: string, index: number): void {
    this.motionManager?.play(group, index);
  }

  setEmotion(emotion: string): void {
    this.model?.setExpression(emotion);
  }

  private onResize(): void {
    const parent = this.canvas.parentElement;
    if (parent) {
      this.app.renderer.resize(
        parent.clientWidth,
        parent.clientHeight
      );
    }
  }

  destroy(): void {
    this.model?.release();
    this.app.destroy(true);
  }
}
```

### 4.4 LipSyncProcessor (å‰ç«¯)

```typescript
export class LipSyncProcessor {
  private audioContext: AudioContext;
  private analyser: AnalyserNode;
  private live2d: Live2DRenderer;
  private isProcessing = false;

  constructor(audioContext: AudioContext, live2d: Live2DRenderer) {
    this.audioContext = audioContext;
    this.live2d = live2d;

    // åˆ›å»ºåˆ†æå™¨
    this.analyser = audioContext.createAnalyser();
    this.analyser.fftSize = 256;
    this.analyser.smoothingTimeConstant = 0.1;
  }

  async playAudioChunk(chunkData: ArrayBuffer): Promise<void> {
    // è§£ç éŸ³é¢‘
    const audioBuffer = await this.audioContext.decodeAudioData(chunkData);

    // åˆ›å»ºæºèŠ‚ç‚¹
    const source = this.audioContext.createBufferSource();
    source.buffer = audioBuffer;

    // è¿æ¥: source -> analyser -> destination
    source.connect(this.analyser);
    this.analyser.connect(this.audioContext.destination);

    // å¯åŠ¨å¤„ç†å¾ªç¯
    this.isProcessing = true;
    this.processLoop();

    // æ’­æ”¾
    source.start();
  }

  private processLoop(): void {
    if (!this.isProcessing) return;

    // è·å–é¢‘ç‡æ•°æ®
    const dataArray = new Uint8Array(this.analyser.frequencyBinCount);
    this.analyser.getByteFrequencyData(dataArray);

    // è®¡ç®—éŸ³é‡ (RMS)
    const rms = this.calculateRMS(dataArray);

    // æ›´æ–°å£å‹
    this.updateMouthOpen(rms);

    // æŒç»­å¾ªç¯
    requestAnimationFrame(() => this.processLoop());
  }

  private calculateRMS(dataArray: Uint8Array): number {
    let sum = 0;
    for (let i = 0; i < dataArray.length; i++) {
      const normalized = dataArray[i] / 255;
      sum += normalized * normalized;
    }
    return Math.sqrt(sum / dataArray.length);
  }

  private updateMouthOpen(volume: number): void {
    // volume èŒƒå›´ 0-1ï¼Œæ˜ å°„åˆ°å£å‹å‚æ•°
    const mouthOpenY = Math.min(volume * 3, 1); // æ”¾å¤§ç³»æ•°ä½¿å˜åŒ–æ›´æ˜æ˜¾
    this.live2d.setParam('ParamMouthOpenY', mouthOpenY);

    // æ ¹æ®éŸ³é‡è°ƒæ•´å˜´å‹ (ç®€å•å…ƒéŸ³æ˜ å°„)
    let mouthForm = 4; // é»˜è®¤ 'o'
    if (volume > 0.4) {
      mouthForm = 0; // 'a'
    } else if (volume > 0.2) {
      mouthForm = 2; // 'u'
    }
    this.live2d.setParam('ParamMouthForm', mouthForm);
  }

  stopProcessing(): void {
    this.isProcessing = false;
    // é‡ç½®å£å‹
    this.live2d.setParam('ParamMouthOpenY', 0);
  }
}
```

## äº”ã€æƒ…æ„Ÿåˆ†æè®¾è®¡

### 5.1 æƒ…æ„Ÿæ£€æµ‹è§„åˆ™

```python
# emotion/analyzer.py
from __future__ import annotations
import re
from typing import Dict, Optional

class EmotionAnalyzer:
    """åŸºäºè§„åˆ™çš„æƒ…æ„Ÿåˆ†æå™¨"""

    # æƒ…æ„Ÿå…³é”®è¯
    EMOTION_KEYWORDS: Dict[str, list[str]] = {
        "happy": [
            "å“ˆå“ˆ", "å˜¿å˜¿", "å˜»å˜»", "å¼€å¿ƒ", "é«˜å…´", "å¤ªå¥½äº†",
            "æ£’", "ä¼˜ç§€", "èµ", "å–œæ¬¢", "çˆ±", "æœ‰è¶£",
            "ğŸ˜„", "ğŸ˜Š", "ğŸ˜‚", "ğŸ¤£", "â¤ï¸", "ğŸ‘",
        ],
        "sad": [
            "éš¾è¿‡", "ä¼¤å¿ƒ", "å“­", "æ‚²ä¼¤", "å¯æƒœ", "é—æ†¾",
            "å¯¹ä¸èµ·", "æŠ±æ­‰", "ä¸å¥½æ„æ€", "ğŸ˜¢", "ğŸ˜­", "ğŸ˜",
        ],
        "angry": [
            "ç”Ÿæ°”", "æ„¤æ€’", "è®¨åŒ", "çƒ¦", "æ»š", "ç¬¨è›‹",
            "æ··è›‹", "ğŸ˜ ", "ğŸ˜¡", "ğŸ‘¿",
        ],
        "surprised": [
            "å“‡", "å¤©å•Š", "å¤©å“ª", "çœŸçš„å—", "ç«Ÿç„¶",
            "å±…ç„¶", "æ²¡æƒ³åˆ°", "ğŸ˜²", "ğŸ˜±", "ğŸ˜®",
        ],
        "thinking": [
            "è®©æˆ‘æƒ³æƒ³", "æ€è€ƒ", "è€ƒè™‘", "åˆ†æ",
            "ç ”ç©¶", "æŸ¥çœ‹", "æ£€æŸ¥",
        ],
    }

    @classmethod
    def analyze(cls, text: str) -> tuple[str, float]:
        """
        åˆ†ææ–‡æœ¬æƒ…æ„Ÿ

        Returns:
            (emotion, intensity): æƒ…æ„Ÿç±»å‹å’Œå¼ºåº¦ (0.0-1.0)
        """
        text_lower = text.lower()

        # è®¡ç®—æ¯ç§æƒ…æ„Ÿçš„å¾—åˆ†
        scores = {}
        for emotion, keywords in cls.EMOTION_KEYWORDS.items():
            score = 0
            for kw in keywords:
                if kw in text_lower:
                    score += 1
            scores[emotion] = score

        # æ‰¾å‡ºæœ€é«˜åˆ†
        if not scores or max(scores.values()) == 0:
            return "neutral", 0.0

        max_emotion = max(scores, key=scores.get)
        max_score = scores[max_emotion]

        # è®¡ç®—å¼ºåº¦ (0.0-1.0)
        intensity = min(max_score / 3, 1.0)

        return max_emotion, intensity

    @classmethod
    def detect_emotion_from_llm(cls, message: str) -> Optional[str]:
        """
        ä» LLM å›å¤ä¸­æ£€æµ‹æƒ…æ„Ÿæç¤º
        ä¾‹å¦‚: (å¾®ç¬‘)ã€(å¹æ°”)ã€(ç‚¹å¤´) ç­‰
        """
        patterns = {
            r"[\(ï¼ˆ][å¾®ç¬‘ç¬‘å¼€å¿ƒæ„‰å¿«][\)ï¼‰]": "happy",
            r"[\(ï¼ˆ][éš¾è¿‡ä¼¤å¿ƒå¹æ°”][\)ï¼‰]": "sad",
            r"[\(ï¼ˆ][ç”Ÿæ°”æ„¤æ€’çš±çœ‰][\)ï¼‰]": "angry",
            r"[\(ï¼ˆ][æƒŠè®¶æƒŠè®¶][\)ï¼‰]": "surprised",
            r"[\(ï¼ˆ][æ€è€ƒæƒ³æƒ³åˆ†æ][\)ï¼‰]": "thinking",
        }

        for pattern, emotion in patterns.items():
            if re.search(pattern, message):
                return emotion

        return None
```

### 5.2 Live2D è¡¨æƒ…å‚æ•°æ˜ å°„

```python
# emotion/mapper.py
from __future__ import annotations

class Live2DParamMapper:
    """æƒ…æ„Ÿåˆ° Live2D å‚æ•°çš„æ˜ å°„"""

    # ä¸åŒè¡¨æƒ…çš„å‚æ•°è°ƒæ•´ (ç›¸å¯¹äºé»˜è®¤å€¼)
    EMOTION_PARAMS: dict[str, dict[str, float]] = {
        "neutral": {
            "ParamEyeLOpen": 1.0,
            "ParamEyeROpen": 1.0,
            "ParamBrowLY": 0.0,
            "ParamBrowRY": 0.0,
            "ParamBrowLAngle": 0.0,
            "ParamBrowRAngle": 0.0,
            "ParamMouthForm": 2.0,
        },
        "happy": {
            "ParamEyeLOpen": 0.8,
            "ParamEyeROpen": 0.8,
            "ParamBrowLY": -0.3,
            "ParamBrowRY": -0.3,
            "ParamBrowLAngle": 0.2,
            "ParamBrowRAngle": 0.2,
            "ParamMouthForm": 0.0,
            "ParamCheek": 0.3,
        },
        "sad": {
            "ParamEyeLOpen": 0.6,
            "ParamEyeROpen": 0.6,
            "ParamBrowLY": 0.3,
            "ParamBrowRY": 0.3,
            "ParamBrowLAngle": -0.2,
            "ParamBrowRAngle": -0.2,
            "ParamMouthForm": 4.0,
        },
        "angry": {
            "ParamEyeLOpen": 0.7,
            "ParamEyeROpen": 0.7,
            "ParamBrowLY": 0.4,
            "ParamBrowRY": 0.4,
            "ParamBrowLAngle": -0.3,
            "ParamBrowRAngle": 0.3,
            "ParamAngleX": -10.0,
            "ParamAngleY": 5.0,
        },
        "surprised": {
            "ParamEyeLOpen": 1.5,
            "ParamEyeROpen": 1.5,
            "ParamBrowLY": -0.5,
            "ParamBrowRY": -0.5,
            "ParamMouthForm": 3.0,
        },
    }

    @classmethod
    def get_params(cls, emotion: str, intensity: float = 1.0) -> dict[str, float]:
        """
        è·å–æƒ…æ„Ÿå¯¹åº”çš„å‚æ•°

        Args:
            emotion: æƒ…æ„Ÿç±»å‹
            intensity: å¼ºåº¦ 0.0-1.0

        Returns:
            å‚æ•°å­—å…¸
        """
        base = cls.EMOTION_PARAMS.get(emotion, cls.EMOTION_PARAMS["neutral"])

        # æ ¹æ®å¼ºåº¦è°ƒæ•´
        return {
            param: value * intensity
            for param, value in base.items()
        }
```

## å…­ã€å‰ç«¯ç»„ä»¶è®¾è®¡

### 6.1 ç»„ä»¶æ ‘ç»“æ„

```
App
â”œâ”€â”€ Live2DContainer
â”‚   â”œâ”€â”€ Live2DCanvas
â”‚   â”‚   â””â”€â”€ PixiJS Canvas
â”‚   â””â”€â”€ LoadingSpinner
â”‚
â”œâ”€â”€ ChatContainer
â”‚   â”œâ”€â”€ MessageList
â”‚   â”‚   â””â”€â”€ MessageBubble[] (ç”¨æˆ·æ¶ˆæ¯ + åŠ©æ‰‹æ¶ˆæ¯)
â”‚   â””â”€â”€ TypingIndicator (æ€è€ƒä¸­æ˜¾ç¤º)
â”‚
â”œâ”€â”€ InputContainer
â”‚   â”œâ”€â”€ TextInput
â”‚   â”œâ”€â”€ VoiceInputButton
â”‚   â””â”€â”€ SendButton
â”‚
â””â”€â”€ SettingsPanel (å¯æŠ˜å )
    â”œâ”€â”€ ModelSelector
    â”œâ”€â”€ VolumeSlider
    â”œâ”€â”€ LipSyncToggle
    â””â”€â”€ DebugInfo
```

### 6.2 æ ¸å¿ƒç»„ä»¶

```typescript
// App.tsx
export function App() {
  const [state, dispatch] = useNakariState();
  const live2d = useLive2D(state.live2d.model);
  const ws = useWebSocket();

  return (
    <div className="app">
      <Live2DContainer
        live2d={live2d}
        state={state.nakari}
      />

      <ChatContainer
        messages={state.conversation.messages}
        isTyping={state.nakari.state === 'thinking'}
      />

      <InputContainer
        onSend={(text) => ws.send({ type: 'user_text', payload: { content: text } })}
        disabled={state.nakari.state === 'thinking'}
      />

      <SettingsPanel
        config={state.config}
        onUpdate={(config) => dispatch({ type: 'UPDATE_CONFIG', config })}
      />
    </div>
  );
}

// Live2DContainer.tsx
interface Live2DContainerProps {
  live2d: Live2DRenderer;
  state: NakariState['nakari'];
}

export function Live2DContainer({ live2d, state }: Live2DContainerProps) {
  const canvasRef = useRef<HTMLCanvasElement>(null);

  useEffect(() => {
    if (canvasRef.current && !live2d.isLoaded) {
      live2d.init(canvasRef.current);
    }
  }, [live2d]);

  // çŠ¶æ€å˜åŒ–æ—¶è§¦å‘åŠ¨ç”»
  useEffect(() => {
    switch (state.state) {
      case 'thinking':
        live2d.playMotion('thinking', 0);
        break;
      case 'idle':
        live2d.playMotion('idle', 0);
        break;
      case 'speaking':
        live2d.playMotion('speaking', 0);
        break;
    }
  }, [state.state, live2d]);

  return (
    <div className="live2d-container">
      <canvas ref={canvasRef} />
    </div>
  );
}

// MessageBubble.tsx
interface MessageBubbleProps {
  role: 'user' | 'assistant';
  content: string;
  emotion?: string;
  timestamp: number;
}

export function MessageBubble({ role, content, emotion, timestamp }: MessageBubbleProps) {
  return (
    <div className={`message-bubble ${role}`}>
      {emotion && <EmotionIndicator emotion={emotion} />}
      <p className="message-content">{content}</p>
      <span className="message-time">
        {new Date(timestamp).toLocaleTimeString()}
      </span>
    </div>
  );
}
```

## ä¸ƒã€é”™è¯¯å¤„ç†ä¸é‡è¿

### 7.1 WebSocket é‡è¿ç­–ç•¥

```typescript
export class WebSocketClient {
  private ws: WebSocket | null = null;
  private reconnectAttempts = 0;
  private maxReconnectAttempts = 5;
  private reconnectDelay = 1000;
  private reconnectTimer: number | null = null;

  async connect(url: string): Promise<void> {
    try {
      this.ws = new WebSocket(url);
      this.setupHandlers();
      this.reconnectAttempts = 0;
    } catch (error) {
      this.scheduleReconnect(url);
    }
  }

  private setupHandlers(): void {
    if (!this.ws) return;

    this.ws.onopen = () => {
      console.log('WebSocket connected');
      this.reconnectAttempts = 0;
    };

    this.ws.onclose = (event) => {
      console.log('WebSocket closed:', event.code, event.reason);
      this.scheduleReconnect();
    };

    this.ws.onerror = (error) => {
      console.error('WebSocket error:', error);
    };
  }

  private scheduleReconnect(url?: string): void {
    if (this.reconnectAttempts >= this.maxReconnectAttempts) {
      console.error('Max reconnect attempts reached');
      return;
    }

    if (this.reconnectTimer) {
      clearTimeout(this.reconnectTimer);
    }

    const delay = this.reconnectDelay * Math.pow(2, this.reconnectAttempts);
    console.log(`Reconnecting in ${delay}ms...`);

    this.reconnectTimer = window.setTimeout(() => {
      this.reconnectAttempts++;
      if (url) {
        this.connect(url);
      }
    }, delay);
  }

  disconnect(): void {
    if (this.reconnectTimer) {
      clearTimeout(this.reconnectTimer);
    }
    if (this.ws) {
      this.ws.close();
      this.ws = null;
    }
  }
}
```

### 7.2 éŸ³é¢‘ç¼“å†²ä¸å¹³æ»‘

```typescript
export class AudioBuffer {
  private buffer: ArrayBuffer[] = [];
  private playing = false;
  private sequence = 0;
  private expectedSequence = 0;

  addChunk(chunk: AudioChunk): void {
    this.buffer.push(chunk.data);
    this.sequence = chunk.sequence;
  }

  hasGap(): boolean {
    return this.sequence > this.expectedSequence && this.expectedSequence > 0;
  }

  getExpectedSequence(): number {
    return this.expectedSequence;
  }

  advanceSequence(): void {
    this.expectedSequence++;
  }

  clear(): void {
    this.buffer = [];
    this.sequence = 0;
    this.expectedSequence = 0;
  }
}
```

## å…«ã€æ€§èƒ½ä¼˜åŒ–

### 8.1 éŸ³é¢‘ä¼ è¾“ä¼˜åŒ–

```python
# ä½¿ç”¨ Opus ç¼–ç é™ä½å¸¦å®½
import opus

class AudioEncoder:
    def __init__(self, sample_rate: int = 24000, bitrate: int = 64000):
        self.encoder = opus.Encoder(sample_rate, 1, opus.APPLICATION_AUDIO)
        self.encoder.bitrate = bitrate

    def encode(self, pcm_data: bytes) -> bytes:
        """ç¼–ç éŸ³é¢‘ä¸º Opus æ ¼å¼"""
        return self.encoder.encode(pcm_data, frame_size=960)
```

### 8.2 Live2D æ¸²æŸ“ä¼˜åŒ–

```typescript
// ä½¿ç”¨ requestAnimationFrame çš„èŠ‚æµç‰ˆæœ¬
class ThrottledRenderer {
  private lastFrame = 0;
  private targetFPS = 60;
  private frameInterval = 1000 / this.targetFPS;

  render(callback: () => void): void {
    const now = performance.now();
    const elapsed = now - this.lastFrame;

    if (elapsed > this.frameInterval) {
      callback();
      this.lastFrame = now - (elapsed % this.frameInterval);
    }
  }
}
```

## ä¹ã€æµ‹è¯•ç”¨ä¾‹

### 9.1 åç«¯æµ‹è¯•

```python
# tests/test_frontend_adapter.py
import pytest
from nakari.frontend_adapter.input import WebSocketInput
from nakari.frontend_adapter.output import WebSocketOutput
from nakari.frontend_adapter.audio_interceptor import AudioStreamInterceptor

@pytest.mark.asyncio
async def test_websocket_input_creates_event(mailbox):
    ws = MockWebSocket()
    ws.add_message('{"type": "user_text", "content": "hello"}')

    input_adapter = WebSocketInput(mailbox, config)
    await input_adapter.handle_message(ws)

    events = mailbox.list_events()
    assert len(events) == 1
    assert events[0].type == EventType.USER_TEXT
    assert events[0].content == "hello"

@pytest.mark.asyncio
async def test_audio_interceptor_broadcasts():
    original = MockTTSBackend()
    broadcaster = MockAudioBroadcaster()
    interceptor = AudioStreamInterceptor(original, broadcaster)

    async for chunk in interceptor.synthesize_stream("test"):
        pass

    assert broadcaster.broadcast_count > 0
```

### 9.2 å‰ç«¯æµ‹è¯•

```typescript
// tests/LipSyncProcessor.test.ts
describe('LipSyncProcessor', () => {
  it('should calculate RMS correctly', () => {
    const processor = new LipSyncProcessor(mockAudioContext, mockLive2D);
    const dataArray = new Uint8Array([128, 128, 128, 128]);
    const rms = processor['calculateRMS'](dataArray);
    expect(rms).toBeCloseTo(0.5, 1);
  });

  it('should update mouth parameter based on volume', () => {
    const live2d = mockLive2D;
    const processor = new LipSyncProcessor(mockAudioContext, live2d);

    processor['updateMouthOpen'](0.8);
    expect(live2d.setParam).toHaveBeenCalledWith('ParamMouthOpenY', 1.0);
  });
});
```

## åã€éƒ¨ç½²é…ç½®

### 10.1 Docker Compose æ‰©å±•

```yaml
# docker-compose.yml (æ–°å¢)
services:
  # ... ç°æœ‰æœåŠ¡ ...

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      - VITE_API_URL=ws://localhost:8000/api/ws
      - VITE_HTTP_URL=http://localhost:8000/api
    volumes:
      - ./frontend/src:/app/src
      - ./frontend/assets:/app/assets
    command: npm run dev -- --host
    depends_on:
      - api

  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    ports:
      - "8000:8000"
    environment:
      - NAKARI_API_ENABLED=true
      - NAKARI_API_HOST=0.0.0.0
      - NAKARI_API_PORT=8000
    depends_on:
      - neo4j
```

### 10.2 Nginx é…ç½®

```nginx
# nginx.conf
server {
    listen 80;
    server_name localhost;

    # WebSocket å‡çº§
    location /api/ws {
        proxy_pass http://api:8000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_read_timeout 86400;
    }

    # HTTP API
    location /api/ {
        proxy_pass http://api:8000;
        proxy_set_header Host $host;
    }

    # å‰ç«¯é™æ€æ–‡ä»¶
    location / {
        proxy_pass http://frontend:3000;
        proxy_set_header Host $host;
    }
}
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2026-02-20
**ä½œè€…**: nakari project
